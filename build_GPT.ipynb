{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZlrmR+xO02Ad54yhL8NvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LIYunzhe1408/GPT_Transformer_spell_out/blob/main/build_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset\n",
        "Download a tiny dataset and peak it"
      ],
      "metadata": {
        "id": "n1JCLMZ_KMFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmM2yv-VJ8P-",
        "outputId": "e069c8ed-f7c7-45a0-d14d-d78e1fd6d5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-17 05:41:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-11-17 05:41:08 (13.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "zYeTcpYJKXzF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHuyL2DJKh0Y",
        "outputId": "298c6538-3a60-46be-e368-55feb3132a14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMtcLP1KmBo",
        "outputId": "d557a3b2-9980-4240-bc34-a0666d2380c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab & Tokenization"
      ],
      "metadata": {
        "id": "Y2e-525W4IVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the vocabulary of this dataset.\n",
        "\n",
        "All the unique characters in this text. Possible characters the model can see and imitate."
      ],
      "metadata": {
        "id": "Zd7502AXLXpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))\n",
        "vocab_size = len(characters)\n",
        "print(''.join(characters))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "9b1kqzRP4QiD",
        "outputId": "66f7bf80-17b0-4351-9074-4fb927faad95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the raw text to some sequence of integers according to the vocabulary of all possible elements.\n",
        "\n",
        "> Short sequences of integers with very large vocabularies V.S. Long sequences of integers with small vocabularies\n",
        "\n",
        "`hii there`\n",
        "* Basic mapping with all characters: 2, 46, 46, 1, 52, 2, 12, 54, 12 with only 65 vocabularies.\n",
        "* Tiktoken: 71, 4178, 612 with 50257 vocabularies.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDihfZybLkhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = {i: ch for i, ch in enumerate(characters)}\n",
        "stoi = {ch: i for i, ch in itos.items()}\n",
        "encode = lambda string: [stoi[ch] for ch in string] # Encoder: take a string, output a list of integers\n",
        "decode = lambda integers: ''.join([itos[i] for i in integers]) # Decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii, there\")))"
      ],
      "metadata": {
        "id": "Eops5pw25TXy",
        "outputId": "257b6760-56c8-4e9e-8cf4-e8d0b9c41f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii, there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the text and store it in to a torch.tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "id": "ml8lF1Ee-rOr",
        "outputId": "e709401c-7051-4fbf-9aae-93edc47c386f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train and val\n",
        "- First 90% of the `training` dataset\n",
        "- Last 10% as the `validation` dataset"
      ],
      "metadata": {
        "id": "s9iO10qkV2i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "id": "_kLANqkP_Igu",
        "outputId": "b3e305ff-fee6-4148-ef69-afd0c8c23c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader\n",
        "1. Time dimension\n",
        "  - How many characters the model can receive: block size.\n",
        "  - The model can generate answers starting from as little as one context character to block_size length. Because during the training process, the model is set to train across limit. Over that limit, the model will chunk the data and will not be able to receive."
      ],
      "metadata": {
        "id": "GIhbwpzyXzF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(context, \"--->\", target.item())"
      ],
      "metadata": {
        "id": "o6WkHM_m_bnH",
        "outputId": "2f4a968e-98cb-4323-9947-083873059942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18]) ---> 47\n",
            "tensor([18, 47]) ---> 56\n",
            "tensor([18, 47, 56]) ---> 57\n",
            "tensor([18, 47, 56, 57]) ---> 58\n",
            "tensor([18, 47, 56, 57, 58]) ---> 1\n",
            "tensor([18, 47, 56, 57, 58,  1]) ---> 15\n",
            "tensor([18, 47, 56, 57, 58,  1, 15]) ---> 47\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47]) ---> 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Batch dimension\n",
        "  - Multiple chunks of text will stack up in a single tensor. This is done for efficiency to keep the gpus busy to process data parallel.\n",
        "  - Batch size: how many independent sequences will we process in parallel?\n",
        "  - Block size: what is the maximum context length for predictions?"
      ],
      "metadata": {
        "id": "yv2cRvzQZuXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  index_x = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  xb = torch.stack([data[i:i+block_size] for i in index_x])\n",
        "  yb = torch.stack([data[i+1:i+block_size+1] for i in index_x])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"Inputs: \")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"Targets: \")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "# Given multiple/single character as context\n",
        "# Output one next character\n",
        "for b in range(batch_size):\n",
        "  print(f\"The {b}-th sample in this batch\")\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"When the input is {context} ---> targert is {target.item()}\")\n",
        "  print(\"--------------------------------------\")"
      ],
      "metadata": {
        "id": "2lW8KtQIA4ul",
        "outputId": "2cb61dec-aa9d-4e40-f6fc-2d4e7c96eb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "The 0-th sample in this batch\n",
            "When the input is tensor([24]) ---> targert is 43\n",
            "When the input is tensor([24, 43]) ---> targert is 58\n",
            "When the input is tensor([24, 43, 58]) ---> targert is 5\n",
            "When the input is tensor([24, 43, 58,  5]) ---> targert is 57\n",
            "When the input is tensor([24, 43, 58,  5, 57]) ---> targert is 1\n",
            "When the input is tensor([24, 43, 58,  5, 57,  1]) ---> targert is 46\n",
            "When the input is tensor([24, 43, 58,  5, 57,  1, 46]) ---> targert is 43\n",
            "When the input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) ---> targert is 39\n",
            "--------------------------------------\n",
            "The 1-th sample in this batch\n",
            "When the input is tensor([44]) ---> targert is 53\n",
            "When the input is tensor([44, 53]) ---> targert is 56\n",
            "When the input is tensor([44, 53, 56]) ---> targert is 1\n",
            "When the input is tensor([44, 53, 56,  1]) ---> targert is 58\n",
            "When the input is tensor([44, 53, 56,  1, 58]) ---> targert is 46\n",
            "When the input is tensor([44, 53, 56,  1, 58, 46]) ---> targert is 39\n",
            "When the input is tensor([44, 53, 56,  1, 58, 46, 39]) ---> targert is 58\n",
            "When the input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) ---> targert is 1\n",
            "--------------------------------------\n",
            "The 2-th sample in this batch\n",
            "When the input is tensor([52]) ---> targert is 58\n",
            "When the input is tensor([52, 58]) ---> targert is 1\n",
            "When the input is tensor([52, 58,  1]) ---> targert is 58\n",
            "When the input is tensor([52, 58,  1, 58]) ---> targert is 46\n",
            "When the input is tensor([52, 58,  1, 58, 46]) ---> targert is 39\n",
            "When the input is tensor([52, 58,  1, 58, 46, 39]) ---> targert is 58\n",
            "When the input is tensor([52, 58,  1, 58, 46, 39, 58]) ---> targert is 1\n",
            "When the input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) ---> targert is 46\n",
            "--------------------------------------\n",
            "The 3-th sample in this batch\n",
            "When the input is tensor([25]) ---> targert is 17\n",
            "When the input is tensor([25, 17]) ---> targert is 27\n",
            "When the input is tensor([25, 17, 27]) ---> targert is 10\n",
            "When the input is tensor([25, 17, 27, 10]) ---> targert is 0\n",
            "When the input is tensor([25, 17, 27, 10,  0]) ---> targert is 21\n",
            "When the input is tensor([25, 17, 27, 10,  0, 21]) ---> targert is 1\n",
            "When the input is tensor([25, 17, 27, 10,  0, 21,  1]) ---> targert is 54\n",
            "When the input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) ---> targert is 39\n",
            "--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n",
        "- bigram language model\n",
        "- loss\n",
        "- generation"
      ],
      "metadata": {
        "id": "_44sG4VCjnSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)\n",
        "print(xb.shape)"
      ],
      "metadata": {
        "id": "sDR2W2X7jjdO",
        "outputId": "cc014df1-adfe-477f-dae3-f83064f6da75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "torch.Size([4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B, T) tensors of integers\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "    # Just inference\n",
        "    if targets is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx: (B, T)\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      # For Biagram model, we just see the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # Apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # Sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # Append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "idx = m.generate(idx, max_new_tokens=100)[0]\n",
        "print(decode(idx.tolist()))"
      ],
      "metadata": {
        "id": "kAwnmLyoUSa_",
        "outputId": "cf3e6e33-5856-4460-ff11-3cb0c3edcc08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "for step in range(10000):\n",
        "  # Sample a batch of data\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  # Evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "jIw9kanrY9xk",
        "outputId": "9b8786f5-8e85-49b3-d188-06cc5424a383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4491610527038574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "idx = m.generate(idx, max_new_tokens=300)[0]\n",
        "print(decode(idx.tolist()))"
      ],
      "metadata": {
        "id": "xY0lSYoKarBC",
        "outputId": "9a974d28-1782-44a1-cf06-780060720f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Se I m tis be h bed BRe hucul bew Jal s helas!\n",
            "\n",
            "I's, golld.\n",
            "Thondreed ait?\n",
            "Pr t:\n",
            "Tod.\n",
            "\n",
            "IIViray:\n",
            "ADuto beebof brs ICE sar t-by h k f mbus.\n",
            "Yo wha moan\n",
            "HENThitigo thes;\n",
            "Awnay wenthunt! oofay nt thet o--\n",
            "Mestheaitloraimf name nseat hilom moriss thethe t o selobe or sh oer conture wf imbunosoudedsuiss e\n"
          ]
        }
      ]
    }
  ]
}