{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2QjKwTUdZ/9VRKT+3zAQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LIYunzhe1408/GPT_Transformer_spell_out/blob/main/build_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset\n",
        "Download a tiny dataset and peak it"
      ],
      "metadata": {
        "id": "n1JCLMZ_KMFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmM2yv-VJ8P-",
        "outputId": "651a46a6-3293-4611-9943-81db4d24a30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 04:42:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-07-28 04:42:19 (17.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "zYeTcpYJKXzF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHuyL2DJKh0Y",
        "outputId": "ce1d61bb-ea2c-428f-9781-b009369fdc5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMtcLP1KmBo",
        "outputId": "b69bf630-5ead-481c-e2eb-b7dc2d952eee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the vocabulary of this dataset.\n",
        "\n",
        "All the unique characters in this text. Possible characters the model can see and imitate."
      ],
      "metadata": {
        "id": "Zd7502AXLXpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWZTqzwOKyj8",
        "outputId": "211b45f7-66ba-4f39-ede6-75a1b4db3d3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the input text\n",
        "Convert the raw text to some sequence of integers according to the vocabulary of all possible elements.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDihfZybLkhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kUdEemyLLgj",
        "outputId": "6aa5e263-73bc-4d8c-a8c7-57ccc991ef92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 1, 58, 46, 43, 56, 43]\n",
            "hi there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYvfXd8IVdst",
        "outputId": "e9852004-7eca-47ff-dc6e-86b38e9ff6cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set train and val\n",
        "- First 90% of the `training` dataset\n",
        "- Last 10% as the `validation` dataset"
      ],
      "metadata": {
        "id": "s9iO10qkV2i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "gLvl6-pVV1vs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader\n",
        "1. Time dimension\n",
        "  - How many characters the model can receive: block size.\n",
        "  - The model can generate answers starting from as little as one context character to block_size length. Because during the training process, the model is set to train across limit. Over that limit, the model will chunk the data and will not be able to receive."
      ],
      "metadata": {
        "id": "GIhbwpzyXzF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnovlsgZY5eR",
        "outputId": "c96cafa1-c42e-4487-c498-acc6af38514e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'When input context is {context}, the target is {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yND8vQphYZEq",
        "outputId": "fc417c55-9747-4174-cdd1-10a51d70601f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input context is tensor([18]), the target is 47\n",
            "When input context is tensor([18, 47]), the target is 56\n",
            "When input context is tensor([18, 47, 56]), the target is 57\n",
            "When input context is tensor([18, 47, 56, 57]), the target is 58\n",
            "When input context is tensor([18, 47, 56, 57, 58]), the target is 1\n",
            "When input context is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
            "When input context is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
            "When input context is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Batch dimension\n",
        "  - Multiple chunks of text will stack up in a single tensor. This is done for efficiency to keep the gpus busy to process data parallel.\n",
        "  - Batch size: how many independent sequences will we process in parallel?\n",
        "  - Block size: what is the maximum context length for predictions?"
      ],
      "metadata": {
        "id": "yv2cRvzQZuXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  index_x = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "  x = torch.stack([data[i:i+block_size] for i in index_x])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in index_x])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"Inputs:\")\n",
        "print(\"Each one of below is a chunk of the traning set\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"Targets:\")\n",
        "print(\"Give us the correct answer for every single position inside x\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print(\"---------\")\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f'When input context is {context}, the target is {target}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mALjMOF5ZXlu",
        "outputId": "7e24a55f-9da2-48e5-b8f9-be69b3f5d75b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "Each one of below is a chunk of the traning set\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Targets:\n",
            "Give us the correct answer for every single position inside x\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "---------\n",
            "When input context is tensor([24]), the target is 43\n",
            "When input context is tensor([24, 43]), the target is 58\n",
            "When input context is tensor([24, 43, 58]), the target is 5\n",
            "When input context is tensor([24, 43, 58,  5]), the target is 57\n",
            "When input context is tensor([24, 43, 58,  5, 57]), the target is 1\n",
            "When input context is tensor([24, 43, 58,  5, 57,  1]), the target is 46\n",
            "When input context is tensor([24, 43, 58,  5, 57,  1, 46]), the target is 43\n",
            "When input context is tensor([24, 43, 58,  5, 57,  1, 46, 43]), the target is 39\n",
            "When input context is tensor([44]), the target is 53\n",
            "When input context is tensor([44, 53]), the target is 56\n",
            "When input context is tensor([44, 53, 56]), the target is 1\n",
            "When input context is tensor([44, 53, 56,  1]), the target is 58\n",
            "When input context is tensor([44, 53, 56,  1, 58]), the target is 46\n",
            "When input context is tensor([44, 53, 56,  1, 58, 46]), the target is 39\n",
            "When input context is tensor([44, 53, 56,  1, 58, 46, 39]), the target is 58\n",
            "When input context is tensor([44, 53, 56,  1, 58, 46, 39, 58]), the target is 1\n",
            "When input context is tensor([52]), the target is 58\n",
            "When input context is tensor([52, 58]), the target is 1\n",
            "When input context is tensor([52, 58,  1]), the target is 58\n",
            "When input context is tensor([52, 58,  1, 58]), the target is 46\n",
            "When input context is tensor([52, 58,  1, 58, 46]), the target is 39\n",
            "When input context is tensor([52, 58,  1, 58, 46, 39]), the target is 58\n",
            "When input context is tensor([52, 58,  1, 58, 46, 39, 58]), the target is 1\n",
            "When input context is tensor([52, 58,  1, 58, 46, 39, 58,  1]), the target is 46\n",
            "When input context is tensor([25]), the target is 17\n",
            "When input context is tensor([25, 17]), the target is 27\n",
            "When input context is tensor([25, 17, 27]), the target is 10\n",
            "When input context is tensor([25, 17, 27, 10]), the target is 0\n",
            "When input context is tensor([25, 17, 27, 10,  0]), the target is 21\n",
            "When input context is tensor([25, 17, 27, 10,  0, 21]), the target is 1\n",
            "When input context is tensor([25, 17, 27, 10,  0, 21,  1]), the target is 54\n",
            "When input context is tensor([25, 17, 27, 10,  0, 21,  1, 54]), the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n",
        "- bigram language model\n",
        "- loss\n",
        "- generation"
      ],
      "metadata": {
        "id": "_44sG4VCjnSR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDR2W2X7jjdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}